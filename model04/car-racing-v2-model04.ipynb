{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8eea6db-85c6-41ec-86e6-c33876b5cd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import copy, random, os, subprocess\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b986a31-fe27-454a-b700-5899f7b4cb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'model04'\n",
    "BUCKET = 'gs://etsuji-car-racing-v2'\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "!gsutil mb -c regional -l us-west1 $BUCKET\n",
    "!gsutil ls $BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a539d906-85d3-487e-a0d9-c111e8abbec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base model\n",
    "class QValue:\n",
    "    def __init__(self):\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        cnn_input = layers.Input(shape=(96, 96, 6), name='cnn_input')\n",
    "        cnn1 = layers.Conv2D(32, (5, 5), padding='same',\n",
    "                         use_bias=True, activation='relu',\n",
    "                        name='cnn1')(cnn_input)\n",
    "        pool1 = layers.MaxPooling2D((2, 2), name='pool1')(cnn1)\n",
    "        cnn2 = layers.Conv2D(64, (5, 5), padding='same',\n",
    "                         use_bias=True, activation='relu',\n",
    "                        name='cnn2')(pool1)\n",
    "        pool2 = layers.MaxPooling2D((2, 2), name='pool2')(cnn2)\n",
    "\n",
    "        cnn_flatten = layers.Flatten(name='flatten')(pool2)\n",
    "        action_input = layers.Input(shape=(5,), name='action_input')\n",
    "        combined = layers.concatenate([cnn_flatten, action_input], name='concat')\n",
    "        hidden1 = layers.Dense(1024, activation='relu', name='dense1')(combined)\n",
    "        hidden2 = layers.Dense(512, activation='relu', name='dense2')(hidden1)\n",
    "        q_value = layers.Dense(1, name='output')(hidden2)\n",
    "\n",
    "        model = models.Model(inputs=[cnn_input, action_input], outputs=q_value)\n",
    "        model.compile(loss='mse')\n",
    "        return model\n",
    "\n",
    "    def get_action(self, state):\n",
    "        states = []\n",
    "        actions = []\n",
    "        for a in range(5):\n",
    "            states.append(np.array(state))\n",
    "            action_onehot = np.zeros(5)\n",
    "            action_onehot[a] = 1\n",
    "            actions.append(action_onehot)\n",
    "  \n",
    "        q_values = self.model.predict([np.array(states), np.array(actions)])\n",
    "        optimal_action = np.argmax(q_values)\n",
    "        return optimal_action, q_values[optimal_action][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db3f866-7954-4ed8-b413-edc8726dfc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_frames(o0, o1):\n",
    "    return np.r_[o0.transpose(), o1.transpose()].transpose() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e2758d-6498-41a9-af72-ee0f359fb94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_episode(environ, q_value, epsilon):\n",
    "    episode = []\n",
    "    o0 = environ.reset()\n",
    "    o1 = copy.deepcopy(o0)\n",
    "    total_r = 0\n",
    "\n",
    "    if epsilon > 0:\n",
    "        keep_count = 3\n",
    "    else:\n",
    "        keep_count = 1\n",
    "\n",
    "    c = 0\n",
    "    while True:\n",
    "        if c % keep_count == 0: # Get new action\n",
    "            if np.random.random() < epsilon:\n",
    "                a = np.random.randint(5)\n",
    "            else:\n",
    "                a, _ = q_value.get_action(join_frames(o0, o1))\n",
    "        c += 1\n",
    "        o_new, r, done, inf = environ.step(a)                \n",
    "        total_r += r\n",
    "\n",
    "        # Terminate episode when total reward becomes negative\n",
    "        if total_r < 0:\n",
    "            done = 1\n",
    "\n",
    "        if done:\n",
    "            # Terminal state is to achive more than 990 or get out of the field.\n",
    "            if total_r > 990 or r < -99:\n",
    "                episode.append((join_frames(o0, o1), a, r, None))\n",
    "            break\n",
    "        else:\n",
    "            episode.append((join_frames(o0, o1), a, r, join_frames(o1, o_new)))\n",
    "        o0, o1 = o1, o_new\n",
    "\n",
    "    print('epsilon={}, episode length={}, total rewards={}'.format(episode, len(episode), total_r))\n",
    "    return episode, total_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a401e5d9-9703-426e-957d-92cda5a7c34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(environ, q_value, epsilon, checkpoint=0):\n",
    "    if checkpoint > 0:\n",
    "        filename = 'car-racing-v2-{}-{}.hd5'.format(MODEL_NAME, checkpoint)\n",
    "        subprocess.run(['gsutil', 'cp', '{}/{}/{}'.format(BUCKET, MODEL_NAME, filename), './'])\n",
    "        print('load model {}'.format(filename))\n",
    "        q_value.model = models.load_model(filename)\n",
    "        os.remove(filename)\n",
    "\n",
    "    experience = []\n",
    "    good_experience = []\n",
    "    best_r = [-100, -100, -100]\n",
    "\n",
    "    for n in range(checkpoint + 1, checkpoint + 1000):\n",
    "        print('iteration {}'.format(n))\n",
    "\n",
    "        total_len = 0\n",
    "        if n % 3 == 0:\n",
    "            print('Testing the current performance...')\n",
    "            episode, total_r = get_episode(environ, q_value, epsilon=0)\n",
    "            with open('result.txt', 'a') as f:\n",
    "                f.write('{},{},{},{}\\n'.format(n, epsilon, len(episode), total_r))\n",
    "            filename = 'car-racing-v2-{}-{}.hd5'.format(n, MODEL_NAME)\n",
    "            q_value.model.save(filename, save_format='h5')\n",
    "            subprocess.run(['gsutil', '-m', 'cp',\n",
    "                            '{}'.format(filename), '{}/{}/'.format(BUCKET, MODEL_NAME)])\n",
    "            os.remove(filename)\n",
    "            experience += episode\n",
    "            total_len += len(episode)\n",
    "\n",
    "        while total_len < 500:\n",
    "            episode, total_r = get_episode(environ, q_value, epsilon)\n",
    "            total_len += len(episode)\n",
    "            experience += episode\n",
    "\n",
    "            # Keep the top 3 episodes\n",
    "            if total_r > min(best_r):\n",
    "                best_r = best_r[1:] + [total_r]\n",
    "                good_experience += episode\n",
    "                if len(good_experience) > 999 * 3:\n",
    "                    good_experience = good_experience[-333 * 3:]\n",
    "\n",
    "            \n",
    "        if len(experience) > 999 * 5: # remember last 5 episodes\n",
    "            experience = experience[-999 * 5:]\n",
    "\n",
    "        epsilon = (epsilon - 0.2) * 0.99 + 0.2\n",
    "\n",
    "        print('Training the model...')\n",
    "        # Use latest episode + past episodes (sampling) + top 3 episode (sampling)\n",
    "        latest_experience = experience[-total_len:]\n",
    "        past_experience = experience[:-total_len]\n",
    "        examples = latest_experience + \\\n",
    "            random.sample(past_experience, min(len(past_experience), 999)) + \\\n",
    "            random.sample(good_experience, min(len(good_experience), 999))\n",
    "        \n",
    "        # Show some statistics\n",
    "        print('experience length={}'.format(len(experience)))\n",
    "        print('number of examples={}'.format(len(examples)))\n",
    "        print('best total reward = ', best_r)\n",
    "        np.random.shuffle(examples)\n",
    "                        \n",
    "        states, actions, labels = [], [], []\n",
    "        for state, a, r, state_new in examples:\n",
    "            states.append(np.array(state))\n",
    "\n",
    "            action_onehot = np.zeros(5)\n",
    "            action_onehot[a] = 1\n",
    "            actions.append(action_onehot)\n",
    "            \n",
    "            if state_new is None:   # Terminal state\n",
    "                q_new = 0\n",
    "            else:\n",
    "                _, q_new = q_value.get_action(state_new)\n",
    "            labels.append(np.array(r + q_new))\n",
    "\n",
    "        hist = q_value.model.fit(\n",
    "            [np.array(states), np.array(actions)], np.array(labels),\n",
    "            batch_size=50, epochs=10, verbose=0)\n",
    "        print('loss = {}'.format(hist.history['loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c407f1a-ef5f-4569-bdcd-2d6197a78c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CarRacing-v2\", continuous=False)\n",
    "q_value = QValue()\n",
    "q_value.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad96c84-0beb-4d87-8f78-e18aeab5bff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(env, q_value, epsilon=1.0, checkpoint=0)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m94"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
